# Multimodal AI Essentials

Welcome to the "[Multimodal AI Essentials](https://www.oreilly.com/live-events/multimodal-ai-essentials/0642572002285)" code repository! In this repo, we will learn how multimodal AI merges text, image, and audio for smarter models.

Much of the code in these sessions will be featured in the 2nd edition of [my latest book on LLMs](https://www.amazon.com/quick-start-guide-language-models/dp/0138199191):

<div style="text-align: center;">
    <a href="https://www.amazon.com/quick-start-guide-language-models/dp/0138199191">
        <img src="images/book.png" width="100" alt="A Quick Start Guide to LLMs">
    </a>
</div>

so if you're itching for more, check it out and please leave a rating/review to tell me what you thought :)

For even more, check out my [Expert Playlist](https://learning.oreilly.com/playlists/2953f6c7-0e13-49ac-88e2-b951e11388de)!

## Prerequisites


- Intermediate - Advanced Python Skills: Comfort with Python is crucial as we'll be using it throughout the course to interact with Hugging Face tools and integrate NLP into practical examples.

- Foundational Machine Learning Knowledge: You should have an understanding of core machine learning principles, as weâ€™ll build upon these concepts when exploring advanced NLP techniques.nologies in dynamic and evolving data environments.

## Installation

1. Clone this repository to your local machine.
2. Ensure you have set the following api keyes:

- OpenAI key

You're all set to explore the notebooks!

## Usage - Jupyter Notebooks

This project contains several Jupyter notebooks each focusing on a specific topic:

1. **[Intro to Multimodality](https://colab.research.google.com/drive/1zYSzDuYFa_cbRlti3scUjfmvradK8Sf4?usp=sharing)**: An introduction to multimodality with CLIP and SHAP-E

	- **[Whisper](https://colab.research.google.com/drive/1KxLWEEBtgix4zgP52pnxlIoJrZ8sHEYC?usp=sharing)**: An introduction to using Whisper for audio transcription

	- **[Llava](https://colab.research.google.com/drive/1IwNAz1Ee4YUSRNCU-SOsa7FS8Q2vmpoL?usp=sharing)**: Using an open source mult-turn multimodal engine

2. **Visual Q/A**
	- Constructing and Training our model
		- [Local](notebooks/constructing_a_vqa_system.ipynb)
		- [Colab](https://colab.research.google.com/drive/1zvbruS1DvFrVgXjNouSrrF9-PphKLWWl?usp=sharing) 

	- Using our VQA system
		- [Local](notebooks/using_our_vqa.ipynb)
		- [Colab](https://colab.research.google.com/drive/16GOBndQuIBO-UfXdpPte-PXaZS2nsW1H?usp=sharing)

## Contributing
Pull requests are welcome. For major changes, please open an issue first to discuss what you would like to change.

## Book time with me on Intro!
If you have questions, I'm available on [Intro](https://intro.co/sinanozdemir) :) 

<div style="text-align: center;">
    <a href="https://intro.co/sinanozdemir">
        <img src="images/intro.png" width="300" alt="Book time with me on Intro">
    </a>
</div>
